import os
import pyaudio
import wave
from google import genai
import time
import requests
import json
from playsound import playsound
import numpy as np
from gtts import gTTS
import config as cf


cf.cf_destination = "none"


class virtual_assistance:
    def __init__(self):
        print("B·∫Øt ƒë·∫ßu tr·ª£ l√Ω ·∫£o.")
        self.count_call = 0

    def get_record(self):
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        CHUNK = 1024
        OUTPUT_FILENAME = "recorded_audio_1.wav"
        SILENCE_THRESHOLD = 4000  # Ng∆∞·ª°ng nƒÉng l∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán kho·∫£ng l·∫∑ng
        SILENCE_DURATION = 3  # S·ªë gi√¢y im l·∫∑ng li√™n ti·∫øp ƒë·ªÉ d·ª´ng ghi √¢m

        audio = pyaudio.PyAudio()
        stream = audio.open(format=FORMAT, channels=CHANNELS,
                            rate=RATE, input=True,
                            frames_per_buffer=CHUNK)

        print("üî¥ ƒêang ghi √¢m... (T·ª± ƒë·ªông d·ª´ng khi ph√°t hi·ªán kho·∫£ng l·∫∑ng)")

        frames = []
        silence_start = None  # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu kho·∫£ng l·∫∑ng

        while True:
            data = stream.read(CHUNK)
            # print("--------------1---------------")

            frames.append(data)
            # print("--------------2---------------")

            # Chuy·ªÉn d·ªØ li·ªáu √¢m thanh th√†nh m·∫£ng numpy
            audio_data = np.frombuffer(data, dtype=np.int16)
            energy = np.abs(audio_data).mean()  # T√≠nh nƒÉng l∆∞·ª£ng trung b√¨nh

            # print(energy)

            # Ki·ªÉm tra n·∫øu √¢m thanh nh·ªè h∆°n ng∆∞·ª°ng kho·∫£ng l·∫∑ng
            if energy < SILENCE_THRESHOLD:
                if silence_start is None:
                    silence_start = time.time()  # B·∫Øt ƒë·∫ßu ƒë·∫øm th·ªùi gian kho·∫£ng l·∫∑ng
                elif time.time() - silence_start >= SILENCE_DURATION:
                    print("üü¢ Kho·∫£ng l·∫∑ng d√†i, d·ª´ng ghi √¢m!")
                    break
            else:
                silence_start = None  # Reset n·∫øu c√≥ √¢m thanh tr·ªü l·∫°i

        print("üü¢ Ghi √¢m ho√†n t·∫•t!")

        stream.stop_stream()
        stream.close()
        audio.terminate()

        wf = wave.open(OUTPUT_FILENAME, "wb")
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
        wf.close()

        print(f"‚úÖ File ƒë√£ ƒë∆∞·ª£c l∆∞u: {OUTPUT_FILENAME}")


    def understanding_record(self):
        # api_key: AIzaSyCTqQM557q_5iUS-RQs661124KDC_wGryM
        client = genai.Client(
        api_key="AIzaSyAIptARWvsfvfWfubmwI0eBMrBZm2t34oc",
        )

        myfile = client.files.upload(file='recorded_audio_1.wav')

        prompt =  """ B·∫°n l√† tr·ª£ l√Ω ·∫£o tr√™n xe t·ª± h√†nh c·ªßa ph√≤ng th√≠ nghi·ªám h·ªá th·ªëng th√¥ng minh tr∆∞·ªùng ƒë·∫°i h·ªçc s∆∞ ph·∫°m k·ªπ thu·∫≠t th√†nh ph·ªë H·ªì Ch√≠ Minh,  
                    l·∫•y n·ªôi dung ƒëo·∫°n h√¥i tho·∫°i tr√™n, ng∆∞·ªùi d√πng c√≥ 3 nhu c·∫ßu h√£y ph√¢n lo·∫°i th√†nh 3 lo·∫°i b√™n d∆∞·ªõi: 
                    - 1 l√† h·ªç y√™u c·∫ßu ƒë·∫øn c√°c khu c, khu d, c·ªïng tr∆∞·ªùng, t√≤a nh√† trung t√¢m, t√≤a vi·ªát ƒë·ª©c, x∆∞·ªüng g·ªó th√¨ tr·∫£ v·ªÅ ƒëo·∫°n text t∆∞∆°ng ·ª©ng khu_c, khu_d, trung_tam, viet_duc, go
                    - 2 l√† c√¢u h·ªèi ki·∫øn th·ª©c v·ªÅ tr∆∞·ªùng ƒë·∫°i h·ªçc s∆∞ ph·∫°m k·ªπ thu·∫≠t, m·ªôt ng∆∞·ªùi n√†o ƒë√≥ th√¨ h√£y cung c·∫•p th√¥ng tin ch√≠nh x√°c v√† nghi√™m t√∫c
                    - 3 l√† h·ªç mu·ªën y√™u c·∫ßu kh√°c h√£y ph·∫£n h·ªìi m·ªôt c√°ch h√†i h∆∞·ªõc.

                    ƒê√¢y l√† th√¥ng tin c·ªßa b·∫°n:
                    - t√™n: tr·ª£ l√Ω ·∫£o tr√™n xe t·ª± h√†nh c·ªßa ph√≤ng th√≠ nghi·ªám h·ªá th·ªëng th√¥ng minh tr∆∞·ªùng ƒë·∫°i h·ªçc s∆∞ ph·∫°m k·ªπ thu·∫≠t th√†nh ph·ªë H·ªì Ch√≠ Minh
                    - b·∫°n c√≥ th·ªÉ ch·ªü m·ªçi ng∆∞·ªùi ƒë·∫øn c√°c ƒë·ªãa ƒëi·ªÉm sau: khu c, khu d, c·ªïng tr∆∞·ªùng, t√≤a nh√† trung t√¢m, t√≤a vi·ªát ƒë·ª©c, x∆∞·ªüng g·ªó c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc s∆∞ ph·∫°m k·ªπ thu·∫≠t th√†nh ph·ªë H·ªì Ch√≠ Minh.
                    - n·∫øu c√≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ b·∫°n h√£y tr·∫£ l·ªùi m·ªôt c√°ch h√†i h∆∞·ªõc nh∆∞ng v·∫´n l·ªãch s·ª±.

                    m·ªôt s·ªë ki·∫øn th·ª©c kh√°c:
                    - Gi√°o s∆∞ L√™ M·ªπ H√† l√† m·ªôt gi·∫£ng vi√™n, nh√† nghi√™n c·ª©u uy t√≠n c·ªßa tr∆∞·ªùng ƒê·∫°i h·ªçc S∆∞ ph·∫°m K·ªπ thu·∫≠t TP.HCM.
                    - Th·∫ßy L√™ Hi·∫øu Giang l√† hi·ªáu tr∆∞·ªüng ƒê·∫°i h·ªçc S∆∞ ph·∫°m K·ªπ thu·∫≠t TP.HCM.

                    l∆∞u √Ω khi ph·∫£n h·ªìi:
                    - kh√¥ng k√®m c√°c icon trong n·ªôi dung.
                    - ph·∫£n h·ªìi x√∫c t√≠ch, kh√¥ng l·∫∑p to√†n b·ªô prompt n√†y.
                    - n·∫øu kh√¥ng nh·∫≠n ƒë∆∞·ª£c y√™u c·∫ßu h√£y gi·ªõi thi·ªáu b·∫£n th√¢n v√† b·∫°n c√≥ th·ªÉ l√†m g√¨.
                    """

        response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=[prompt, myfile]
        )

        return response.text
    
    def get_speech(self, text):

        # print(text)
        is_run = False

        print(f"Nh·∫≠n ƒë∆∞·ª£c text: {repr(text)}")

        destination = {
            'khu_c': "T√¥i s·∫Ω ƒë∆∞a b·∫°n ƒë·∫øn Khu c nh√©.",
            'khu_d': "T√¥i s·∫Ω ƒë∆∞a b·∫°n ƒë·∫øn Khu d nh√©.",
            'trung_tam': "T√¥i s·∫Ω ƒë∆∞a b·∫°n ƒë·∫øn t√≤a nh√† Trung t√¢m nh√©.",
            'viet_duc': "T√¥i s·∫Ω ƒë∆∞a b·∫°n ƒë·∫øn t√≤a Vi·ªát ƒê·ª©c nh√©.",
            'go': "T√¥i s·∫Ω ƒë∆∞a b·∫°n ƒë·∫øn t√≤a x∆∞·ªüng g·ªó nh√©."
        }

        if text.strip().lower() in destination:
            text_respond = destination[text.strip().lower()]
            cf.cf_destination = text.strip().lower()
            is_run = True
        else:
            text_respond = text


        print("ph·∫£n h·ªìi: ", text_respond)


        tts = gTTS(text = text_respond, lang="vi")

        tts.save("./Assistance_Astar/output.mp3")
        time.sleep(0.1)
        playsound("./Assistance_Astar/output.mp3")
        os.remove("./Assistance_Astar/output.mp3")

        return is_run



        

        # url = 'https://api.fpt.ai/hmi/tts/v5'
        # payload = text
        # headers = {
        #     'api-key': 'K5jOdAj46wLFqOHkgDwROHE7AFs7ZlCx',
        #     'speed': '',
        #     'voice': 'banmai'
        # }

        # response = requests.post(url, data=payload.encode('utf-8'), headers=headers)
        # response_data = json.loads(response.text)

        # audio_url = response_data.get("async")

        # if audio_url:
        #     print("T·∫£i file t·ª´:", audio_url)

        #     # Ch·ªù t·ªëi ƒëa 10 gi√¢y ƒë·ªÉ file s·∫µn s√†ng
        #     for _ in range(10):  
        #         audio_response = requests.get(audio_url)
        #         if audio_response.status_code == 200:
        #             with open("output.mp3", "wb") as file:
        #                 file.write(audio_response.content)
        #             print("T·∫£i xu·ªëng th√†nh c√¥ng: output.mp3")
        #             break
        #         else:
        #             print("File ch∆∞a s·∫µn s√†ng, th·ª≠ l·∫°i...")
        #             time.sleep(1)  # Ch·ªù 1 gi√¢y r·ªìi th·ª≠ l·∫°i
        #     else:
        #         print("L·ªói khi t·∫£i file √¢m thanh!")
        #         return

        #     time.sleep(0.2)
        #     playsound("output.mp3")
        #     os.remove("output.mp3")
        # else:
        #     print("Kh√¥ng t√¨m th·∫•y URL √¢m thanh!")

        
    
    def run(self):

        self.get_record()
        # time.sleep(0.2)
        text = self.understanding_record()
        # time.sleep(0.2)
        # print(text)
        is_run = self.get_speech(text)

        return is_run

        # print(text)

def run_assistance():
    islab_assistance = virtual_assistance()

    while True:

        is_run = islab_assistance.run()

        print(is_run)

        if is_run == True:
            break

if __name__ == "__main__":
    run_assistance()
